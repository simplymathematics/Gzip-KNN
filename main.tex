\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{algorithm2e}
\usepackage{algpseudocode}
\usepackage{subcaption}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\newcommand{\tl}[1]{\textit{[{\color{red}#1}]}}
\newcommand{\cm}[1]{\textit{{\color{blue}#1}}}
\newcommand{\erik}[1]{\textit{[{\color{brown}Erik: #1}]}}
\newcommand{\mr}[1]{\textit{[{\color{brown}Mohammed: #1}]}}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Compression Distance for Anomaly Detection}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Charles Meyers}
% \email{cmeyers@cs.umu.se}
% \affiliation{%
%   \institution{Umeå University}
%   \city{Umeå}
%   \country{Sweden}
% }
% \author{Aaron P. MacSween}
% \email{publishing@cryptography.dog}
% \affiliation{
%     \institution{null}
%     \city{na}
%     \country{undefined}
% }
% \author{Tommy Löfstedt}
% \email{tommy@cs.umu.se}
% % \affiliation{%
% %   \institution{Umeå University}
% %   \city{Umeå}
% %   \country{Sweden}
% % }
% \author{Erik Elmroth}
% \email{elmroth@cs.umu.se}
% % \affiliation{%
% %   \institution{Umeå University}
% %   \city{Umeå}
% %   \country{Sweden}
% % }

% % \affiliation{%
% %   \institution{Umeå University}
% %   \city{Umeå}
% %   \country{Sweden}
% %   \country{USA}
% %   \postcode{43017-6221}
% % }

\maketitle

\begin{abstract}
  Machine learning have proven remarkable performance across a wide variety of domains, but nevertheless fail to adversaries during model training or deployment. 
  Recent developments have focused on incredibly complex architectures with long run-times, specific hardware requirements, and models that leak private information to anyone with access to the API.  
  However, the use of compression algorithms in conjunction with clustering algorithms has proven remarkably successful at text classification, excelling in few-show circumstances and across a wide-variety of datasets. This makes it ideal for a private anomaly detection algorithm. In this paper, we show how a client-side approach to anomaly detection is safer by design.
  In addition, we demonstrate the efficacy of this technique in the domain of computer security --- using datasets spanning computer processes, Twitter bots, intrusion attacks, denial of service attacks, and log file outlier detection.
  We offer advice for improving the model latency and evaluate NCD in the wider context of kernel metrics to show its broad efficacy.
    
\end{abstract}

\section{Introduction}

Despite their efficacy across many domains, modern machine learning methods often use very large models that require large numbers of samples to train \cite{desislavov2021compute}. The exchange of this data often creates privacy and security risks \cite{}. For example, several attacks against ML systems have been proposed, targeting the model during training \cite{biggio_poisoning_2013}, prediction \cite{biggio_evasion_2013,deepfool,carlini_towards_2017}, or deployment \cite{distributed_attacks,santos2021universal}. Even when access to a model is limited only to the classification labels, it is possible to subvert the model \cite{hopskipjump}, reverse engineer the decision boundary \cite{deepfool}, determine the model weights \cite{jagielski2020high} or infer the class-membership of new samples \cite{bentley2020quantifying}. This raises profound questions for safety critical systems \cite{meyers} and legal questions about access and control of the underlying data \cite{mitrou2018data,marks2023ai}. One solution to the problem of adversaries is to treat the model as we would treat a session token or any other private data --- by keeping it encrypted locally and providing no access in either direction to the internet. However, this lack of data sharing means that our method must work well when trained on a small number of samples to minimise the run-time costs and maximize the efficacy for individual users.  Luckily, Jiang et al.\cite{jiang2022less} proposed a parameter-free text classification algorithm that exploits the compression algorithm, DEFLATE, to compress and then classify strings that has shown very strong performance against several benchmark datasets. In this study, we explore the underlying theory of kernel methods, offer run-time improvements over the original\cite{jiang2022less} GZIP-KNN classifier, demonstrate the efficacy of our run-time optimized method on a variety of datasets comprised of a variety of data types to provide a plausible model for private and client-side anomaly detection.  

\subsection{Motivations}
 We have several motivations for this work. Firstly, Jiang's analysis relies on many thousands of samples and it's unclear how well it performs in the few-shot circumstances. While follow-up research examined topics like image classification \cite{opitz2023gzip}, chemical classification \cite{weinreich2023parameter}, and text classification \cite{nishida2011tweet}, it has not been explored in the context of anomaly detection. Furthermore, if we can minimize the run-time storage and computation requirements, then we can fulfill our goal of deploying this method entirely client-side.

\subsection{Contributions}

In this study, we:

\begin{itemize}
    \item Explore the theory behind normalized compression distance as a pseudo-metric
    \item Demonstrate how private model deployment is safer by design(Sec.~\ref{threat})
    \item Offer run-time improvements over the GZIP-KNN classifier (Sec.~\ref{improvements})
    \item Provide a live example of the model in javascript as well as a scikit-learn implementation
    \item Demonstrate the efficacy of the GZIP-KNN classifier across several anomaly detection datasets
    \item Demonstrate the efficacy of the run-time optimizations
\end{itemize}

\subsection{Definitions}
\paragraph{Distance}
Distance refers to any norm or pseudo-norm for measuring the proximity of one sample to another. 
% \paragraph{Benign vs Adversarial}
\paragraph{Failure Rate}
We will define the failure rate
\paragraph{Survival Time}

\section{Background}

In the sections below, we outline the definition of a metric space and the connection to normalized compression distance, briefly k-Nearest Neighbor (kNN) classifiers, and give a high-level overview of the gzip compression algorithm.

\subsection{Measures of Distance}
There are many different distance measures. Jiang et al. examined only the noise-compression distance (see Sec.~\ref{ncd}), but this method is identical to other kernel methods \cite{} that use measures of distance to cluster samples into classes. For strings, several measures of distance are routine and available via the \texttt{levenshtein} package \cite{levenshtein}. Our choice of string metrics is outlined in Sec.~\ref{string_metrics}.


% \subsubsection{Normalized Information Distance}
%  Kolmogorov Complexity
% Not actually computable
% Instead we approximate it.
% One method is...




\subsubsection{Normalized Compression Distance}
\label{ncd}
The normalized compression distance (NCD) is defined as:
\begin{equation}
    \text{NCD}(x, y) = \frac{\mathcal{C}(xy) - \min[\mathcal{C}(x), \mathcal{C}(y)]}{\max[\mathcal{C}(x), \mathcal{C}(y)]},
\end{equation}
where $\mathcal{C}(z)$ is the length (in bits) of the compressed form of the data $z$ using a compression algorithm (\textit{e.g.} DEFLATE), and $xy$ denotes the concatenation of strings $x$ and $y$. It has been used for classification tasks many times \cite{opitz2023gzip,weinreich2023parameter,nishida2011tweet,jiang2022less}.
\subsubsection{Other measures of string distance}
\label{string_metrics}
In addition to the NCD metric defined above, we examined several other measures of string similarity, widely used in natural language processing (NLP) and these are briefly outline below.
% String Distances
\begin{itemize}
    \item \textit{Levenshtein:} the "edit distance" or minimum number of single-character edits to transform one string into another~\cite{navarro2001guided}.
    \item \textit{Ratio:} is one minus Levenshtein distance divided by the total length of the strings~\cite{levenshtein}.
    \item \textit{SeqRatio:} identical to "Ratio", but calculated on sequences of strings by taking the single input string and splitting it into a list around white-space characters~\cite{levenshtein}.
    \item \textit{Hamming:} is the number of character positions where two strings differ, but is only defined for strings of equal length, leading to sparse distance matricies~\cite{hamming_distance}.
    \item \textit{Jaro:} is a measure of similarity between two strings, taking into account the number of identical characters as well as the number of transposed characters~\cite{jaro}.
    \item \textit{Jaro-Winkler:} extends the above by incorporating a scaling factor to give weight to sub-strings that occur more frequently~\cite{jaro}. 
\end{itemize}



\subsubsection{Metric Spaces}

A metric space is defined as some measure of distance between the constituent components of some space, usually called points.  For points $x,y$ in a space, separated by distance $d(x,y)$, $d$ is a true distance metric if and only if the following are true~\cite{metrics}. For the sake of the reader, each identity is given a label (denoted in the parentheses). 

\label{metric_spaces}
\begin{itemize}
    \item $d(x, y) = 0 \iff x = y$ (zero identity)
    \item $d(x, y) \geq 0$ (non-negativity)
    \item $d(x, y) = d(y, x)$ (symmetry)
    \item $d(x, z) \leq d(x, y) + d(y, z)$ (triangle inequality)
\end{itemize}

\subsubsection{Mercer's Theorem}
\cm{Not sure this is necessary}



\subsection{NCD is a pseudo-metric}
\paragraph{Non-negativity} 
\[
d(x, y) \geq 0
\]
The distance between any two points is always non-negative. This is true for all of the compressors operating under normal circumstances. However, if one of the input strings $x,y$ is shorter than the minimum header length and is can be found verbatim in the second string, then this value can be negative. Using our python implementation and the strings $x=, y=$, we found a value of \cm{TODO: grab from the log file when I'm back in Europe since ssh'ing is hard from Japan}. This is also true for all of the string metrics since their domain is $[0,1]$~\cite{metrics,levenshtein}. 


\paragraph{Zero Identity} 
\[
d(x, y) = 0 \iff x = y
\]
The distance between two points is zero if and only if the points are identical. The string metrics are defined such that this is true, however, this needs to be proven for NCD.
Assuming $x = y$, it follows that $\mathcal{C}(x) = \mathcal{C}(y)$ for loss-less compressors.
Since the compression algorithms will replace the matching string, $y$, with a token comprised of $x$, there will be a single additional Huffman coded letter in $\mathcal{C}(xy)$ than in $\mathcal{C}(y)$. Calling the length of that  string $\ell$, it's easy to see that:
$$
\mathcal{C}(xy) = \mathcal{C}(y) + \ell.
$$
where $C$ is the fixed length of the compressed block ($\geq 1$).
Substituting this into the definition of NCD yields
$$
NCD(x,y) = \frac{\mathcal{C}(y) + \ell - \mathcal{C}(y)}{\mathcal{C}(y)} = \frac{\ell}{C(y)}.
$$
Therefore it is clear that this is not zero, but some constant. We propose a solution to this problem in Algorithm~\ref{alg:enforced_symmetry}. However, one other issue remains. If $y$ is a string contained verbatim in $x$ then the NCD is zero, violating the identity. However, by sorting the strings at run-time by length (making the longer one $x$), it is possible to circumvent this since this behaviour is asymmetrical. 

% It's not clear if this \textit{should} be resolved since the marginal new information in a repeated string is, in fact, 0.  


\paragraph{Symmetry} 
\[
d(x, y) = d(y, x)
\]
The distance between \( x \) and \( y \) is the same as the distance between \( y \) and \( x \). While this is true for the string metrics, this is not necessarily the case for the compressor based NCD measures, we show, empirically, that we can enforce this using Algorithm~\ref{alg:assumed_symmetry}. The pros and cons of this are discussed in ~\ref{results}.

\paragraph{Triangle inequality} 
\[
d(x, z) \leq d(x, y) + d(y, z)
\]
The distance between \( x \) and \( z \) is less than or equal to the sum of the distances between \( x \) and \( y \), and \( y \) and \( z \). While this is true for most randomly generated strings, it's not hard to generate a counter example. For example, if $x = "aaa", y = "a", z = "bbb"$ then the the values are \cm{TODO} and the identity is violated.
% Many of the string metrics do not follow this rule and I can probably think of specific compression examples where this would break down. In general, however, this should hold true for these compressors because they are sub-additive~\cite{}, which means: 
% $$
% \mathcal{C}(xy) \leq \mathcal{C}(x) + \mathcal{C}(y)
% $$
% for some compression function $\mathcal{C}$. Recalling the definition of NCD... TODO


% Hmmm. If I have a string  that is unique in x and y, but shared between them, then the triangle inequality falls apart. 

% \begin{align*}
% x &= abcdef \\
% y &= abcdef \\
% C(x) &= abcdef \rightarrow len(C(x)) = 6  \\
% C(y) &= abcdef \rightarrow len(C(y)) = 6  \\
% C(xy) &= C(abcdefabcdef) \\ &\rightarrow len(C(xy))= len(C(x)) + len(\S) \\
% \end{align*}
% where $\S$ is a token that represents the 2nd `abcdef' string. The length of this token depends on, among other things, the compression algorithm, the sliding window size, the compression level, the length of the full strings, the length of the repeated string, and the chosen encoding of the compressed buffer. However, it is clear that the length of this token is generally not the same length as the uncompressed string (since that would imply the compression doesn't actually compress). 



\subsection{Kernelisation}

% \cm{This section is just some notes. I'm trying to conceptualise how NCD replaces the "kernel trick" that normally runs in O(n) time with something that runs in O(n*m) time (where n is the number of test samples and m is the training samples) Overcoming this run-time disadvantage is the focus of the rest of the improvements/experiments, so talking about it here could motivate those efforts and allow us to highlight the ways in which NCD is almost a \textit{proper} metric if we enforce the symmetry.}
In kernel methods, the kernel function, $k(x_i,x_j)$, takes inputs $x_i, x_j$ and maps them to a value in $\mathbb{R}^\kappa$ where $\kappa$ is the number of output dimensions (\textit{i.e.} classes). This is generally applied to transform the feature space in such a way as to be linearly separable to solve nonlinear problems in the original input space. A number near 0 for samples $x_i, x_j$ indicates similarity for both kernel methods and distance measures. Valid kernel-functions are positive semi-definite, which requires a symmetry about the major diagonal axis and all eigenvalues to be non-negative. While this assumption holds for many of the distance measures, this is not necessarily true for NCD, which could vary slightly depending on which sample is first in the concatenation step ($xy$). However, we propose a solution to this issue in Algorithms~\ref{alg:assumed_symmetry} and ~\ref{alg:enforced_symmetry} and examine the effiacy of either method below.

\subsection{KNN}
The algorithm for k-nearest neighbors is reproduced in Algorithm~\ref{alg:knn}.

\begin{algorithm}
    \caption{GZIP-KNN Classifier}
    \label{alg:knn}
    \begin{algorithmic}
    \Require{Training set $X = \{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}$; test instance $x_{\text{test}}$, }
    \Require{Number of nearest neighbors for estimator, $k$; Norm, pseudo-norm, or distance metric $\mathcal{D}$, a sorting algorithm, $\textrm{Sort}()$}
    \For{instance $(x_i, y_i) \in X$}{
        \[
        d_i = \mathcal{D}(x_{\text{test}}, x_i)
        \]
    }
    \State $ X \leftarrow \textrm{Sort}(X, \mathcal{D}(x_{test}, X)
    $
    \State $y_{test} \leftarrow \{y_i | i \in \textrm{Indices}\}$
    \State $\hat{y}_{\text{test}} = \arg\max_y \sum_{i=1}^{k} \mathbb{I}(y_i = y)$
    where $\mathbb{I}(y_i = y)$ is an indicator function, equal to 1 if $y_i = y$ and 0 otherwise.
    \State \Return Predicted class label for $x_{\text{test}}$, $y_{test}$
    \end{algorithmic}
\end{algorithm}


\subsection{GZIP-KNN}
In this section, we reproduce the GZIP-KNN algorithm proposed by Jiang et al. to inform a run-time analysis of the algorithm and offer insights towards our  improvements that are outlined in Sec~\ref{improvements}. Broadly, this method uses a widely-researched clustering model, $k$-Nearest Neighbors, in conjunction with a  distance metric called \textit{normalized compression distance} (NCD). 


\subsection{GZIP}
    The \texttt{gzip} package is an open-source and widely available software tool for compressing files \cite{gzip} that uses the DEFLATE compression algorithm \cite{deflate}. It uses a sliding window dictionary and Huffman coding \cite{} to compress data into an output buffer by reading the symbols one at a time. Then, it iterates through the data. If the current symbol is in the sliding window dictionary, then it will add it to the output buffer. Otherwise, it finds the longest matching sequence of symbols in the window, encodes them using Huffman coding, and removes the matched symbols from the output buffer, before adding the new key to the sliding window dictionary. The full algorithm can be found in the original paper, here\cite{deflate}. 


\section{Security and Privacy}
\label{security}
In this section, we outline the security and privacy risks associated with deploying machine learning models for anomaly detection. First, we briefly outline the threat model for typical machine learning models in the first subsection before explaining how our private model categorically eliminates most of these problems. 

\subsection{Threat Model}
A typical machine learning pipeline is vulnerable to attacks that target each stage of the machine learning pipeline. Broadly speaking, they come in white- and black-box categories \cite{meyers}. Whitebox attacks like the fast gradient method \cite{fgm} or \cite{deepfool} require access to the model directly while other attacks can succeed with only normal . However, it has been shown that this finding prototypical meta samples from the training set is trivial \cite{}. Likewise, we can then use this class membership data to reverse engineer the model weights \cite{} and loss gradients for a set of (potentially adversarial) examples \cite{}. Even if our attacker only has access to a typical application programming interface (API), the HopSkipJump attack \cite{hopskipjump} has been shown to minimize the number of queries needed to subvert detection at run-time. To illustrate these risks, we have included Fig.~\ref{fig:threat_model}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=.6\textwidth]{images/attack_diagram.pdf}
    \caption{This diagram depicts the various threat models against a typical machine learning model. The blue box depicts the typical ML pipeline training process and the red box reveals how each part of the model pipeline can be targeted by an adversary. The solid black lines follow the flow of data within either the benign (blue) or adversarial (red) pipelines. The solid red lines indicate what kinds of data can be extracted via typical API access and the dotted red lines reveal the kinds of data that can be inserted into a ML pipeline by an adversary. Even if an adversary has}
    \label{fig:threat_model}
\end{figure*}
\label{threat}

% \subsection{Attacks}
% \label{attacks}
% \begin{itemize}
%     \item FGM
%     \item HSJ


% \end{itemize}
% \subsubsection{Attack Related Metrics}
% \paragraph{Minimal Evasion Distance}
% \paragraph{Survival Time}
% % \pargraph{KL_Divergence}
% \paragraph{SHAPr Privacy Metric}
\section{Run-Time Improvements}
\label{improvements}
We can use this model in two different paradigms. In general, the idea is that the number of requisite training samples is very small, so we can run everything client-side, keeping the particulars of the model training private. Another method for distributing the training process, federated learning, trains a model for each user but routinely shares model weights upstream to create a better model for everyone. However, as we outlined in Sec.~\ref{threat}, that opens new attack vectors that would be unsuitable for information the user would like to keep private (\textit{e.g.}, IP address, message contents, or schedule of use). Instead, we propose a few-shot and online methodology for re


\subsection{Real-time, Anomaly Detection}
In some situations, it might be necessary to find anomalies in real-time, which involves special considerations. Namely, we can skip the "training" step entirely and select a number of existing samples from our database for training. These could be selected randomly or according to some time filter in an attempt to only capture new or otherwise unnoticed anomalies. In some cases, it may be that the user doesn't have many samples. However, as we show in the Sec.~\ref{results}, that even small training sets are remarkably effective for this method.



%  Numeric Distances
% \begin{itemize}
%     \item $\ell_0$ (bit-wise distance)
%     \item $\ell_1$ (taxi-driver distance)
%     \item $\ell_2$ (euclidean)
%     \item $\mathscr{\chi}^2$ (weighted difference per entry)
%     \item $\mathscr{L}$ (Laplacian)
%     \item $\mathscr{W}$ (Wasserstein)
% \end{itemize}

\subsubsection{Calculating the Gram matrix}
\label{symmetry}
While many of the above distance metrics are symmetric, this is not-necessarily the case with NCD. However, if we assume this is true most of the time, we can reduce the computational cost of training and/or prediction. Below, we have tested two separate modifications of the 

 Section~\ref{results} shows how this often improves the accuracy of a model and reduces the number of calculations required to find the distance matrix.



\begin{algorithm}
    \begin{algorithmic}
        \Require $X_1$, $X_2$, indexed by $i,j$ respectively
        \Ensure $ \| X_1 \| = \| X_2 \| $
        \State $K_{i,j} \gets 0$ for all $i, j$
        \ForAll{pairs of indices $(i, j) \in \mathcal{D}(X_1, X_2)$ such that $i < j + 1$}
            \State $\mathcal{D}_{ij} \gets \mathcal{D}(X_{1_i}, X_{2_j})$
            \State $S_{ij} \gets \mathcal{D}_{ij}$
            \State $S_{ji} \gets \mathcal{D}_{ij}$
        \State \textbf{Output:} Symmetric distance matrix $S$
    \end{algorithmic}
    \caption{Compute the \textit{Assumed} Symmetric Distance Matrix}
    \label{alg:assumed_symmetry}
\end{algorithm}

\begin{algorithm}
    \begin{algorithmic}
        \Require $X_1$, $X_2$, indexed by $i,j$ respectively
        \Ensure $ \| X_1 \| = \| X_2 \| $
        \State $K_{i,j} \gets 0$ for all $i, j$
        \ForAll{pairs of indices $(i, j) \in \mathcal{D}(X_1, X_2)$ such that $i < j + 1$}
            \State $\mathcal{D}_{ij} \gets \mathcal{D}(X_{1_i}, X_{2_j})$
            \State $S_{ij} \gets \mathcal{D}_{ij}$
            \State $S_{ji} \gets \mathcal{D}_{ij}$
        \State \textbf{Output:} Symmetric distance matrix $S$
    \end{algorithmic}
    \caption{Compute the \textit{Enforced} Symmetric Distance Matrix}
    \label{alg:enforced_symmetry}
\end{algorithm}



\subsubsection{Pre-computing the Compression vector}
Since the compression step requires the most processing time, we can pre-compute the compression distance for our input data as part of a training step, if we so choose. This offers marginal run-time improvements over the original implementation that repeatedly calculated the NCD of all training samples for each example in the test set. 
See Fig.~\ref{fig:sym_train_time} to see the (signficant) decrease in run-time  and the resulting (marginal) change in accuracy when we pre-compress all of the train and test strings in the data. %Note: this is still running. 










\subsubsection{Reducing the Search Space}
\label{best-samples}
In other situations, it is necessary to minimize the number of samples used for evaluation because the size of the user's database is too large to evaluated in a reasonable amount of time. We can use a variety of heuristics to reduce the size of our search space \cite{amal2011survey}. 
\begin{itemize}
    \item \textit{Sum:} After calculating the distance matrix, we sort the distance matrix by the sum of a row and find the $m$ most central samples per class.
    \item \textit{Medoid: }
    \item \textit{Random : } After calculating the distance matrix, we select $m$ random indices per class.
    \item \textit{KNN: } After calculating the distance matrix, we use the KNN algorithm to find the $m$-closest neighbors for each class in the training data.
    \item \textit{SVC: } After calculating the distance matrix, we use a support vector classifier (SVC) to find the indices that define the support-vectors (e.g. the samples that define the class boundaries), then we sort by distance as in \textit{Sum} to find the $m$-most central support vectors to use during the prediction step.
\end{itemize}


\begin{algorithm}
  \caption{Find M-Best Indices (Condensing)}
  \label{alg:m-best-indices}
  \begin{algorithmic}
    \State \textbf{Input:} Sample points $X$, number of best indices $m$
    \State \textbf{Output:} Set of indices $I$ containing the $m$-best points
    \ForAll{$x$ in $X$}
      \State Compute relevance score $r_x$ based on a given criterion
    % \EndFor
    \State $I \gets \text{sortIndices}(r_x)$
    \State \Return top $m$ indices in $I$
  \end{algorithmic}
\end{algorithm}


\subsubsection{Iterative Methods}
We can now treat this as an iterative problem and do a continuous hyperparameter search.


\begin{algorithm}
  \caption{Model Training}
  \label{alg:condensing}
  \begin{algorithmic}
    \Require System state $S$, Number of iterations $N$, Number of best indices $m$, $x_{test}, y_{test}, X_{train}, y_{train}$
    \State \textbf{Output:} Final system state $S$
    \State Initialize system: $S \gets$ baseline state
    \For{$i = 1$ to $N$}
      \State Sample points: $P \gets \text{samplePoints}(S)$
      \State Find m-best indices: $I \gets \text{findMBestIndices}(P, m)$
    \State \Return $m$-best indices, $I$
  \end{algorithmic}
\end{algorithm}






\section{Methods}
Broadly, this section is divided into three sets of experiments. First, we tuned each model to each dataset while examining the effect of sample size and the effect of the enforced symmetry (see:~\ref{symmetry}). Then, after confirming the $\mathcal{O}(nm)$ runtime for number of testing, training samples $n,m$, we examine the effect of various training set condensing methods to reduce the run-time. Then, we verify that this method is ...portable?
...more private/robust? that what?
...capable of doing online learning with an exceptionally small database?



% Next steps:
% Online training with condensing? Or is this just an application we can mention, calling the implementation `out of scope'
% Another dataset? Truthseeker is really bad (Twiitter is actually just very spammy and the dataset abels were made up in like 2012). Could be replaced or contextualized with a better, but similar dataset
%  Examine the degree to which the metric space assumptions are violated
% Compare the privacy/attack-ability using the SHAPr metric, but what's the baseline here? A NN? A KNNs/SVCs/Logistic Models that use something other than gzip as a tokenizer (e.g. Word2Vec). My strong suspcion is that you'll get more bits per sample from word2vec than from gzip since the latter only returns a number and not a vector of tokens.
% As of 27/7, I'm leaning towards the SHAPr metric, since that can be implemented rather quickly on, for example, the best model (but tested on a new random permutations than during training). I should know by 29/7 if this is going to go anywhere
\label{methods}
\subsection{Datasets}
\label{datasets}
We examine several to show that this method works well for a variety of anomaly-detection tasks. The `normal' data were under-sampled so that there was an equal number of normal and anomalous samples. For several of the datasets, anomalies were rare. To overcome the limitation of getting a precise measure of accuracy on a small number of samples, Monte Carlo sampling was employed by testing each configuration across many random permutations of the data. This is known to be asymptotically equivalent to cross-fold validation~\cite{} which is not remotely precise enough when we have only a small number of anomalous data points. Additionally, the test and train sets were sampled in such a way that each configuration was guaranteed to have a uniform distribution of the classes in both sets. To examine the efficacy of this method on numeric, string, and heterogeneous datasets, several standard datasets were examined. The oldest one is KDD-NSL which is a log of system process data for both regular users (labelled 0 or benign) and malicious users (labelled 1 or adversarial). The DDoS IoT dataset ~\cite{} was also included. To prepare this dataset, we combined all classes of attacks with the non-malicious user data, collapsing all attacks into the adversarial (1) label. In addition, the timestamp data was removed to prevent data leakage  (since the malicious and benign data were collected sequentially). Then, in an effort to examine the efficacy of this on text alone, the Truthseeker ~\cite{} was used. While this is normally distributed with metadata, this work only included the ``tweet" and the label provided by the researchers who distribute the data (again, a 0 or 1 corresponding to a regular or malicious user respectively). The examine more heterogeneous data, the SMS Spam dataset was chosen. This combines a short text message with numeric and protocol data to again distinguish between regular SMS messages (labelled with a 0) and spam ones (labelled with a 1). 

\subsection{Models}

After generating 1000 train/test set pairs for each dataset, the distance matrices were calculated and provided to typical \texttt{scikit-learn} classifiers. For each compression algorithm or string metric, both the symmetric (as per  Alg. \ref{alg:assumed_symmetry}) and asymmetric matrices were found and tested. Since these models have several parameters, some of which are drawn from continuous spaces, we used a Tree Parzen Estimator to tune each model for each distance metric. The test hyper-parameters are listed below.

\label{models}
\subsubsection{$k$-KNN}
To use KNN, one must specify a number of nearest neighbors to use for class prediction (see: Algorithm~\ref{alg:knn}). This number was selected from $\{1,3,5,7,11\}$, as choosing small, odd numbers to make ties impossible for binary classifiers. In addition, we tested whether or not to weight the samples uniformly or by distance. 

\subsubsection{$k$-SVC}
Ths \texttt{scikit-learn} SVC implementation provides several kernel methods for transforming the input into a separable feature space. For the sake of simplicity, only the radial basis function (RBF) was tested (it's also known to be a universal function approximator~\cite{}). SVCs require a choice of value for a penalty term and we tested the range $[ 10^{-3}, 10^3]$ and let \texttt{scikit-learn} choose an apporpriately sized $\gamma$ parameter for the RBF function. The "balanced" and null class weight parameters that control the relative importance of a sample were also tested.

\subsubsection{$k$-Logistic}
The \texttt{scikit-learn} provides several parameters for the humble logistic function. Both an $\ell_1$ and $\ell_2$ penalty term were tested as well as a configuration without a penalty. That penalty was sampled from the range $[10^{-3}, 10^3]$. In addition, logistic models with and without intercept parameters were tested. Using the \texttt{saga} solver, we tested stopping criteria tolerances $\in [10^{-4}, 10^{-2}]$
As with the SVC above, the "balanced" and null class weight parameters that control the relative importance of a sample were also tested.

\subsection{Compressors} In addition to \texttt{gzip}, we tried numerous different compression algorithms. 
\begin{itemize}
    \item gzip - widely available, fast compared to lzma/bz2
    \item lzma - more compression, more time
    \item bz2 - balances gzip/lzma
    \item zstd - designed around real-time usage at the expense of compression depth
\end{itemize}

\paragraph{Condensing}
\label{condensing}
We provide theoretical justifications for using secondary methods for determining ideal training set in Section~\ref{improvements}. For the sake of completeness, we tested the mean, medoid, random, KNN, and SVC methods outlined above as well as selecting the $m$-most important samples for each with $m \in$ [10,20,50,100,200,500,1000]. The methods are outlined in full in Sec.~\ref{best-samples}.


\section{Results}
\label{results}

\subsection{Kernelized Classifier Performance}
\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/models_vs_accuracy.pdf}
        \caption{Accuracy across models and datasets.}
        \label{fig:acc_summary}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/models_vs_train_time.pdf}
        \caption{Training Time across models and datasets.}
        \label{fig:train_time_summary}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/models_vs_predict_time.pdf}
        \caption{Prediction Time across models and datasets.}
        \label{fig:pred_time_summary}
    \end{subfigure}
    \caption{Kernelized classifier performance across many datasets, models, and sample sizes. Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys the number of samples provided during training.}
    \label{fig:sample_summary}
\end{figure*}

\subsection{Effect of Symmetry Assumption}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/combined/symmetric_models_vs_accuracy.pdf}
    \caption{Accuracy across models and datasets.}
    \label{fig:symm_acc}
    \caption{Kernelized classifier accuracy across many datasets, and models while examining the effect of the symmetric assumption outlined in Algorithm~\ref{alg:assumed_symmetry}. Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys whether symmetry was assumed (orange) or not (blue).}
    \label{fig:symm_summary}
\end{figure*}
\begin{figure*}
\centering
        \includegraphics[width=\textwidth]{figs/combined/symmetric_models_vs_train_time.pdf}
        \caption{Kernelized classifier training times across many datasets, and models while examining the effect of the symmetric assumption outlined in Algorithm~\ref{alg:assumed_symmetry}. Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys whether symmetry was assumed (orange) or not (blue).}
        \label{fig:sym_train_time}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/combined/symmetric_models_vs_predict_time.pdf}
    \caption{Kernelized classifier inference times across many datasets, and models while examining the effect of the symmetric assumption outlined in Algorithm~\ref{alg:assumed_symmetry}. Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys whether symmetry was assumed (orange) or not (blue).}
    \label{fig:symm_pred_time}
\end{figure*}




\subsection{Database Condensing Methods}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/combined/condensing_methods_vs_accuracy.pdf}
    \caption{Accuracy across models and datasets.}
    \label{fig:cond_acc}
    \caption{Kernelized classifier accuracy across many datasets, models, while examining the effect of different condensing techniques (see Algorithm~\ref{alg:condensing}). Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys whether symmetry was assumed (orange) or not (blue).}
    \label{fig:condensing_summary}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/combined/condensing_methods_vs_train_time.pdf}
    \caption{Kernelized classifier training times across many datasets, models, while examining the effect of different condensing techniques (see Algorithm~\ref{alg:condensing}). Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys whether symmetry was assumed (orange) or not (blue).}
    \label{fig:cond_train_time}
\end{figure*}

% \begin{figure*}
%      \centering
%     \includegraphics[width=\textwidth]{figs/combined/condensing_methods_vs_predict_time.pdf}
%     \caption{Kernelized classifier inference times across many datasets, models, while examining the effect of different condensing techniques (see Algorithm~\ref{alg:condensing}). Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys whether symmetry was assumed (orange) or not (blue).}
%     \label{fig:cond_pred_time}
% \end{figure*}


\section{Considerations}
\label{considerations}
\section{Conclusion}
\label{conclusion}


\bibliographystyle{ieeetr}
\newpage
\bibliography{bibliography}

% \clearpage
% \appendix
% \onecolumn
% \input{appendix-ddos}
% \clearpage
% \input{appendix-sms_spam}
% \clearpage
% \input{appendix-kdd_nsl}
% \clearpage
% \input{appendix-truthseeker}

\end{document}
