\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm2e}


\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Compression Distance for Anomaly Detection}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Charles Meyers}
% \email{cmeyers@cs.umu.se}
% \affiliation{%
%   \institution{Umeå University}
%   \city{Umeå}
%   \country{Sweden}
% }
% \author{Aaron P. MacSween}
% \email{publishing@cryptography.dog}
% \affiliation{
%     \institution{null}
%     \city{na}
%     \country{undefined}
% }
% \author{Tommy Löfstedt}
% \email{tommy@cs.umu.se}
% % \affiliation{%
% %   \institution{Umeå University}
% %   \city{Umeå}
% %   \country{Sweden}
% % }
% \author{Erik Elmroth}
% \email{elmroth@cs.umu.se}
% % \affiliation{%
% %   \institution{Umeå University}
% %   \city{Umeå}
% %   \country{Sweden}
% % }

% % \affiliation{%
% %   \institution{Umeå University}
% %   \city{Umeå}
% %   \country{Sweden}
% %   \country{USA}
% %   \postcode{43017-6221}
% % }

\begin{abstract}
  Machine learning have proven remarkable performance across a wide variety of domains, but nevertheless fail to adversaries during model training or deployment. 
  Recent developments have focused on incredibly complex architectures with long run-times, specific hardware requirements, and models that leak private information to anyone with access to the API.  
  However, the use of compression algorithms in conjunction with clustering algorithms has proven remarkably successful at text classification, excelling in few-show circumstances and across a wide-variety of datasets. This makes it ideal for a private anomaly detection algorithm. In this paper, we show how a client-side approach to anomaly detection is safer by design.
  In addition, we demonstrate the efficacy of this technique in the domain of computer security---using datasets spanning computer processes, Twitter bots, intrusion attacks, denial of service attacks, and log file outlier detection.
  We offer advice for improving the model latency and evaluate NCD in the wider context of kernel metrics to show its broad efficacy.
    
\end{abstract}
\maketitle

\section{Introduction}

Despite their efficacy across many domains, modern machine learning methods often use very large models that require large numbers of samples to train \cite{desislavov2021compute}. The exchange of this data often creates privacy and security risks \cite{}. For example, several attacks against ML systems have been proposed, targeting the model during training \cite{biggio_poisoning_2013}, prediction \cite{biggio_evasion_2013,deepfool,carlini_towards_2017}, or deployment \cite{distributed_attacks,santos2021universal}. Even when access to a model is limited only to the classification labels, it is possible to subvert the model \cite{hopskipjump}, reverse engineer the decision boundary \cite{deepfool}, determine the model weights \cite{jagielski2020high} or infer the class-membership of new samples \cite{bentley2020quantifying}. This raises profound questions for safety critical systems \cite{meyers} and legal questions about access and control of the underlying data \cite{mitrou2018data,marks2023ai}. One solution to the problem of adversaries is to treat the model as we would treat a session token or any other private data-- by keeping it encrypted locally and providing no access in either direction to the internet. However, this lack of data sharing means that our method must work well when trained on a small number of samples to minimize the run-time costs and maximize the efficacy for individual users.  Luckily, Jiang et al.\cite{jiang2022less} proposed a parameter-free text classification algorithm that exploits the compression algorithm, DEFLATE, to compress and then classify strings that has shown very strong performance against several benchmark datasets. In this study, we explore the underlying theory of kernel methods, offer run-time improvements over the original\cite{jiang2022less} GZIP-KNN classifier, demonstrate the efficacy of our run-time optimized method on a variety of datasets comprised of a variety of data types to provide a plausible model for private and client-side anomaly detection.

\subsection{Motivations}
 We have several motivations for this work. Firstly, Jiang's analysis relies on many thousands of samples and it's unclear how well it performs in the few-shot circumstances. While follow-up research examined topics like image classification \cite{opitz2023gzip}, chemical classification \cite{weinreich2023parameter}, and text classification \cite{nishida2011tweet}, it has not been explored in the context of anomaly detection. Furthermore, if we can minimize the run-time storage and computation requirements, then we can fulfill our goal of deploying this method entirely client-side.

\subsection{Contributions}

In this study, we:

\begin{itemize}
    \item Explore the theory behind the gzip-knn 
    \item Demonstrate how private model deployment is safer by design(Sec.~\ref{threat})
    \item Offer run-time improvements over the GZIP-KNN classifier (Sec.~\ref{improvements})
    \item Provide a live example of the model in javascript as well as a scikit-learn implementation
    \item demonstrate the efficacy of the GZIP-KNN classifier across several anomaly detection datasets
    \item We also demonstrate the efficacy of the run-time optimizations
\end{itemize}

\subsection{Definitions}
\paragraph{Distance}
Distance refers to any norm or pseudo-norm for measuring the proximity of one sample to another. 
% \paragraph{Benign vs Adversarial}
\paragraph{Failure Rate}
We will define the failure rate
\paragraph{Survival Time}

\section{Background}

In the sections below, we outline the definition of a metric space and the connection to normalized compression distance, briefly k-Nearest Neighbor (kNN) classifiers, and give a high-level overview of the gzip compression algorithm.

\subsection{Measures of Distance}
There are many different distance measures. Jiang et. al. examined only the noise-compression distance (see Sec.~\ref{ncd}), but this method is identical to other kernel methods \cite{} that use measures of distance to cluster samples into classes. For numeric data, we can use normal scikit-learn implementations of pairwise distances \footnote{
https://scikit-learn.org/stable/modules/classes.html\#module-sklearn.metrics.pairwise or https://scikit-learn.org/stable/modules/generated/} or just kernel metrics\footnote{sklearn.metrics.pairwise.kernel\_metrics.html\#sklearn.metrics.pairwise.kernel\_metrics}--- both of which are highly parallelizable for single- or multi-core jobs. For strings, several measures of distance are routine and available via the \texttt{levenshtein} package \cite{levenshtein}. Our choice of string metrics is outlined in Sec.~\ref{string_metrics}.

% \subsubsection{Definition of metric space}
% \begin{enumerate}
%     \item \textit{Non-negativity:} 
%     \[
%     d(x, y) \geq 0
%     \]
%     The distance between any two points is always non-negative.
    
%     \item \textit{Identity of indiscernibles:} 
%     \[
%     d(x, y) = 0 \iff x = y
%     \]
%     The distance between two points is zero if and only if the points are identical.
    
%     \item \textit{Symmetry:} 
%     \[
%     d(x, y) = d(y, x)
%     \]
%     The distance between \( x \) and \( y \) is the same as the distance between \( y \) and \( x \).
    
%     \item \textbf{Triangle inequality:} 
%     \[
%     d(x, z) \leq d(x, y) + d(y, z)
%     \]
%     The distance between \( x \) and \( z \) is less than or equal to the sum of the distances between \( x \) and \( y \), and \( y \) and \( z \).
% \end{enumerate}


\subsubsection{Normalized Compression Distance}
\label{ncd}
\begin{equation}
    \text{NCD}(x, y) = \frac{\mathcal{C}(xy) - \min[\mathcal{C}(x), \mathcal{C}(y)]}{\max[\mathcal{C}(x), \mathcal{C}(y)]},
\end{equation}
where $\mathcal{C}(z)$ is the length  of the compressed form of the data $z$ using a compression algorithm (\textit{e.g.} DEFLATE), and $xy$ denotes the concatenation of strings $x$ and $y$. It has been used for classification tasks many times \cite{opitz2023gzip,weinreich2023parameter,nishida2011tweet,jiang2022less}.
\subsubsection{Other measures of string distance}
\label{string_metrics}
In addition to the NCD metric defined above, we examined several other measures of string similarity, widely used in natural language processing (NLP) and these are briefly outline below.
% String Distances
\begin{itemize}
    \item \textit{Levenshtein:} the "edit distance" or minimum number of single-character edits to transform one string into another.
    \item \textit{Ratio:} is one minus Levenshtein distance divided by the total length of the strings.
    \item \textit{SeqRatio:} identical to "Ratio", but calculated on sequences of strings by taking the single input string and splitting it into a list around white-space characters.
    \item \textit{Hamming:} is the number of character positions where two strings differ, but is only defined for strings of equal length, leading to sparse distance matricies.
    \item \textit{Jaro:} is a measure of similarity between two strings, taking into account the number of identical characters as well as the number of transposed characters.
    \item \textit{Jaro-Winkler:} extends the above by incorporating a scaling factor to give weight to sub-strings that occur more frequently. 
\end{itemize}

\subsection{Kernelization}
In kernel methods, the kernel function, $k(x_i,x_j)$, takes inputs $x_i, x_j$ and maps them to a value in $\mathbb{R}^\kappa$ where $\kappa$ is the number of output dimensions (\textit{i.e.} classes). This is generally applied to transform the feature space in such a way as to be linearly separable to solve nonlinear problems in the original input space. A number near 0 for samples $x_i, x_j$ indicates similarity for both kernel methods and distance measures. This connection is deeper than that as the radial basis function kernel function can be derived from the Euclidean notion of distance ~\cite{}. Valid kernel-functions are positive semi-definite, which requires a symmetry about the major diagonal axis. While this assumption holds for many of the distance measures, this is not necessarily true for NCD, which could vary slightly depending on which sample is first in the concatenation step ($xy$). However, we propose a solution

\subsection{KNN}
The algorithm for k-nearest neighbors is reproduced in Alg.~\ref{alg:knn}.

\begin{algorithm}
    \begin{algorithmic}[1]
    \Require{Training set $X = \{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}$; test instance $x_{\text{test}}$, }
    \Require{Number of nearest neighbors for estimator, $k$; Norm, pseudo-norm, or distance metric $\mathcal{D}$, a sorting algorithm, $\textrm{Sort}()$}
    
    
    \For{instance $(x_i, y_i) \in X$}{
        \[
        d_i = \mathcal{D}(x_{\text{test}}, x_i)
        \]
    }
    \State $ X \leftarrow \textrm{Sort}(X, \mathcal{D}(x_{test}, X)
    $
    \State $y_{test} \leftarrow \{y_i | i \in \textrm{Indices}\}$
    \State $\hat{y}_{\text{test}} = \arg\max_y \sum_{i=1}^{k} \mathbb{I}(y_i = y)$
    where $\mathbb{I}(y_i = y)$ is an indicator function, equal to 1 if $y_i = y$ and 0 otherwise.
    \State \Return Predicted class label for $x_{\text{test}}$, $y_{test}$
    \EndFor
    \caption{GZIP-KNN Classifier}
    \label{alg:knn}
    \end{algorithmic}
\end{algorithm}


\subsection{GZIP-KNN}
In this section, we reproduce the GZIP-KNN algorithm proposed by Jiang et al. to inform a run-time analysis of the algorithm and offer insights towards our  improvements that are outlined in Sec~\ref{improvements}. Broadly, this method uses a widely-researched clustering model, $k$-Nearest Neighbors, in conjunction with a  distance metric called \textit{normalized compression distance} (NCD). 


\subsection{GZIP}
    The \texttt{gzip} package is an open-source and widely available software tool for compressing files \cite{gzip} that uses the DEFLATE compression algorithm \cite{deflate}. It uses a sliding window dictionary and Huffman coding \cite{} to compress data into an output buffer by reading the symbols one at a time. Then, it iterates through the data. If the current symbol is in the sliding window dictionary, then it will add it to the output buffer. Otherwise, it finds the longest matching sequence of symbols in the window, encodes them using Huffman coding, and removes the matches symbols from the output buffer, before adding the new key to the sliding window dictionary. The full algorithm can be found in the original paper, here\cite{deflate}.


\section{Security and Privacy}
\label{security}
In this section, we outline the security and privacy risks associated with deploying machine learning models for anomaly detection. First, we briefly outline the threat model for typical machine learning models in the first subsection before explaining how our private model categorically eliminates most of these problems. 

\subsection{Threat Model}
A typical machine learning pipeline is vulnerable to attacks that target each stage of the machine learning pipeline. Broadly speaking, they come in white- and black-box categories \cite{meyers}. Whitebox attacks like the fast gradient method \cite{fgm} or \cite{deepfool} require access to the model directly while other attacks can succeed with only normal . However, it has been shown that this finding prototypical meta samples from the training set is trivial \cite{}. Likewise, we can then use this class membership data to reverse engineer the model weights \cite{} and loss gradients for a set of (potentially adversarial) examples \cite{}. Even if our attacker only has access to a typical application programming interface (API), the HopSkipJump attack \cite{hopskipjump} has been shown to minimize the number of queries needed to subvert detection at run-time. To illustrate these risks, we have included Fig.~\ref{fig:threat_model}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=.6\textwidth]{images/attack_diagram.pdf}
    \caption{This diagram depicts the various threat models against a typical machine learning model.}
    \label{fig:threat_model}
\end{figure*}
\label{threat}

% \subsection{Attacks}
% \label{attacks}
% \begin{itemize}
%     \item FGM
%     \item HSJ


% \end{itemize}
% \subsubsection{Attack Related Metrics}
% \paragraph{Minimal Evasion Distance}
% \paragraph{Survival Time}
% % \pargraph{KL_Divergence}
% \paragraph{SHAPr Privacy Metric}
\section{Run-Time Improvements}
\label{improvements}
We can use this model in two different paradigms. In general, the idea is that the number of requisite training samples is very small, so we can run everything client-side, keeping the particulars of the model training private. Another method for distributing the training process, federated learning, trains a model for each user but routinely shares model weights upstream to create a better model for everyone. However, as we outlined in Sec.~\ref{threat}, that opens new attack vectors that would be unsuitable for information the user would like to keep private (\textit{e.g.}, IP address, message contents, or schedule of use). Instead, we propose a few-shot and online methodology for re


\subsection{Real-time, Anomaly Detection}
In some situations, it might be necessary to find anomalies in real-time, which involves special considerations. Namely, we can skip the "training" step entirely and select a number of existing samples from our database for training. These could be selected randomly or according to some time filter in an attempt to only capture new or otherwise unnoticed anomalies. In some cases, it may be that the user doesn't have many samples. However, as we show in the Sec.~\ref{results}, that even small training sets are remarkably effective for this method.



%  Numeric Distances
% \begin{itemize}
%     \item $\ell_0$ (bit-wise distance)
%     \item $\ell_1$ (taxi-driver distance)
%     \item $\ell_2$ (euclidean)
%     \item $\mathscr{\chi}^2$ (weighted difference per entry)
%     \item $\mathscr{L}$ (Laplacian)
%     \item $\mathscr{W}$ (Wasserstein)
% \end{itemize}

\subsubsection{Calculating distance matrix}

While many of the above distance metrics are symmetric, this is not-necessarily the case with NCD. However, if we assume this is true most of the time, we can reduce the computational cost of training and/or prediction. Section~\ref{results} shows how this often improves the accuracy of a model and reduces the number of 



\begin{algorithm}
  \KwData{$X_1$, $X_2$, indexed by $i,j$ respectively.}
  \KwResult{Symmetric distance matrix $S$}
  \BlankLine
  \textbf{Initialize} symmetric matrix $S$ with zeros\;
  \BlankLine
  \ForEach{pair of indices $(i, j) \in \mathcal{D}(X_1, X_2)$ such that $i < j$ }{
    $\mathcal{D}_{ij} \leftarrow \mathcal{D}(X_{1_i}, X_{2_j} )$ \\
    $S_{ij} \leftarrow \matchal{D}_{ij}$ \\
    $S_{ji} \leftarrow \mathcal{D}_{ij}$ \\
  }
  \caption{Compute Symmetric Distance Matrix}
  \label{alg:symmetric-distance-matrix}
\end{algorithm}

% \subsubsection{Removing time-variant noise using autocorellation techniques}

% TODO: talk about arima models


\subsubsection{Pre-computing the Compression vector}
Since the compression step requires the most processing time, we can pre-compute the compression distance for our input data as part of a training step, if we so choose. This offers marginal run-time improvements over the original implementation that repeatedly calculated the NCD of all training samples for each example in the test set. 
See Fig.~\ref{fig:symmetry_time} to see the (signficant) decrease and run-time  and the resulting (marginal) change in accuracy when we pre-compress all of the train and test strings in the data. %Note: this is still running. 










\subsubsection{Reducing the Search Space}
\label{best-samples}
In other situations, it is necessary to minimize the number of samples used for evaluation because the size of the user's database is too large to evaluated in a reasonable amount of time. We can use a variety of heuristics to reduce the size of our search space \cite{amal2011survey}. 
\begin{itemize}
    \item \textbf{Sum:} After calculating the distance matrix, we sort the distance matrix by the sum of a row and find the $m$ most central samples per class.
    \item \textbf{Medoid: }
    \item \textbf{Random : } After calculating the distance matrix, we select $m$ random indices per class.
    \item \textbf{KNN: } After calculating the distance matrix, we use the KNN algorithm to find the $m$-closest neighbors for each class in the training data.
    \item \textbf{SVC: } After calculating the distance matrix, we use a support vector classifier (SVC) to find the indices that define the support-vectors (e.g. the samples that define the class boundaries), then we sort by distance as in \textit{Sum} to find the $m$-most central support vectors to use during the prediction step.
\end{itemize}


\begin{algorithm}
  \caption{Find M-Best Indices (Condensing)}
  \SetAlgoNlRelativeSize{0}
  \SetAlgoNlRelativeSize{-1}
  \SetAlgoNlRelativeSize{-2}
  
  \KwData{Sample points $P$, Number of best indices $m$}
  \KwResult{Set of indices $I$ with the m-best points}
  
  \ForEach{point $p$ in $P$}{
    Calculate a relevance score $r_p$ based on some criterion\;
  }
  
  Sort indices based on relevance scores in descending order: $I \gets$ sortIndices($r_p$)\;
  
  \Return top $m$ indices $I$\;
\end{algorithm}



\subsubsection{Iterative Methods}
We can now treat this as an iterative problem and do a continuous hyperparameter search.



\begin{algorithm}
  \caption{Model Training}
  \label{alg:symmetry}
  \KwData{System state $S$, Number of iterations $N$, Number of best indices $m$, $x_{test}, y_{test}, X_{train}, y_{train}$}
  \KwResult{Final system state $S$}
  
  Initialize system: $S \gets$ baseline state\;
  \For{$i=1$ to $N$}{
    Sample points: $P \gets$ samplePoints($S$)\;
    Find m-best indices: $I \gets$ findMBestIndices($P, m$)\;
  }
  \Return $m$-best indices, $I$\;
\end{algorithm}







\section{Methods}

\subsection{Datasets}
We examine several to show that this method works well for a variety of anomaly-detection tasks.
\begin{itemize}
    \item KDD-NSL (Intrusion Detection)
    \item Truthseeker (Twitter Bot)
    \item SMS Spam
    \item PCAP DDOS
\end{itemize}
\subsection{Models}

\paragraph{$k$-clusters}
To use KNN, one must specify a number of nearest neighbors to use for class prediction (see: Algorithm~\ref{alg:knn}). We tested $k \in \{1,3,5,7,11\}$, choosing small, odd numbers to make ties impossible for binary classifiers.
\paragraph{Compressors:} In addition to \texttt{gzip}, we tried numerous different compression algorithms. 
\begin{itemize}
    \item gzip - widely available, fast compared to lzma/bz2
    \item lzma - more compression, more time
    \item bz2 - balances gzip/lzma
    \item zstd - designed around real-time usage at the expense of compression depth
\end{itemize}

\paragraph{Training Sample Optimization:}
\label{methods}
We provide theoretical justifications for using secondary methods for determining ideal training set in Section~\ref{improvements}. For the sake of completeness, we tested the mean, medoid, random, KNN, and SVC methods outlined above as well as selecting the $m$-most important samples for each with $m \in$ [10,20,50,100,200,500,1000]. The methods are outlined in full in Sec.~\ref{best-samples}.


\section{Results}
\label{results}

\subsection{Kernelized Classifier Performance}
\begin{figure*}[t]
    \centering
    \captionsetup[subfigure]{skip=0pt}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/models_vs_accuracy.pdf}
        \caption{Accuracy across models and datasets.}
        \label{acc_summary}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/models_vs_train_time.pdf}
        \caption{Training Time across models and datasets.}
        \label{train_time_summary}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/models_vs_train_time.pdf}
        \caption{Prediction Time across models and datasets.}
        \label{pred_time_summary}
    \end{subfigure}
    \caption{Kernelized classifier performance across many datasets, models, and sample sizes. Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys the number of samples provided during training.}
    \label{fig:sample_summary}
\end{figure*}

\subsection{Effect of Symmetry Assumption}

\begin{figure*}[t]
    \centering
    \captionsetup[subfigure]{skip=0pt}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/symmetric_models_vs_accuracy.pdf}
        \caption{Accuracy across models and datasets.}
        \label{acc_summary}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/symmetric_models_vs_train_time.pdf}
        \caption{Training Time across models and datasets.}
        \label{train_time_summary}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/symmetric_models_vs_predict_time.pdf}
        \caption{Prediction Time across models and datasets.}
        \label{pred_time_summary}
    \end{subfigure}
    \caption{Kernelized classifier performance across many datasets, models, and assumptions of diagnoal symmetry in the training matrix. Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys whether symmetry was assumed (orange) or not (blue).}
    \label{fig:sample_summary}
\end{figure*}

\subsection{Database Condensing Methods}

\begin{figure*}[t]
    \centering
    \captionsetup[subfigure]{skip=0pt}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/condensing_methods_vs_accuracy.pdf}
        \caption{Accuracy across models and datasets.}
        \label{acc_summary}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/condensing_methods_vs_train_time.pdf}
        \caption{Training Time across models and datasets.}
        \label{train_time_summary}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/combined/condensing_methods_vs_predict_time.pdf}
        \caption{Prediction Time across models and datasets.}
        \label{pred_time_summary}
    \end{subfigure}
    \caption{Kernelized classifier performance across many datasets, models, and assumptions of diagnoal symmetry in the training matrix. Each column corresponds to a different dataset and each row corresponds to a different performance metric, the value of which is on the y-axis. The x-axis in each plot corresponds to a classifier type and the colour conveys whether symmetry was assumed (orange) or not (blue).}
    \label{fig:sample_summary}
\end{figure*}


\section{Considerations}
\label{considerations}
\section{Conclusion}
\label{conclusion}


\bibliographystyle{ieeetr}
\clearpage
\bibliography{acmart}

\end{document}
\endinput
%%
